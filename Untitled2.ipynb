{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22459698-67bd-4aa5-bdb0-0beb12f83b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import openai  # Import OpenAI library\n",
    "\n",
    "# Load NLP Models\n",
    "nlp = spacy.load(\"en_core_web_trf\")  # Better than 'en_core_web_sm'\n",
    "nlp_ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # For better entity similarity matching\n",
    "\n",
    "# Google API Key (Replace this with your own)\n",
    "GOOGLE_API_KEY = \"AIzaSyCKL_3TpCL7mt8dincmwy45WrFaenyosZY\"\n",
    "\n",
    "# OpenAI API Key (Replace this with your own)\n",
    "OPENAI_API_KEY = \"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\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Cache for geocoded locations\n",
    "geo_cache = {}\n",
    "\n",
    "\"\"\"In summary, the \"extract_entities\" function combines the strengths of two different NER approaches \n",
    "(spaCy and a transformer-based model) to extract geographic entities from a given text. \n",
    "By using both methods, the function aims to improve the coverage and accuracy of the entity extraction process.\n",
    "The final output is a list of unique geographic entities found in the input text.\"\"\"\n",
    "\n",
    "\"\"\"def extract_entities(text):\n",
    "    \"\"Extract Named Entities using both spaCy and Transformer-based NER models. \"\n",
    "    entities = set()\n",
    "\n",
    "    # spaCy Named Entity Recognition (NER)\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"GPE\", \"LOC\"]:  # Extract only geographic entities\n",
    "            entities.add(ent.text.strip())\n",
    "\n",
    "    # Transformer-based NER (aggregated output)\n",
    "    transformer_entities = nlp_ner(text)\n",
    "    for ent in transformer_entities:\n",
    "        entities.add(ent['word'].strip())\n",
    "\n",
    "    return list(entities)\"\"\"\n",
    "\n",
    "import openai\n",
    "import json\n",
    "import faiss\n",
    "import pickle\n",
    "import re\n",
    "import helper\n",
    "\n",
    "def extract_entities(text, examples):\n",
    "    \"\"\"\n",
    "    Extract named entities from the given text using OpenAI's language model and similar examples.\n",
    "    \"\"\"\n",
    "    # Convert examples to a formatted string\n",
    "    examples_text = \"\\n\".join([f\"Text: {ex[0]}\\nExtracted Entities: {ex[1]}\" for ex in examples])\n",
    "\n",
    "    # Construct system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    You are tasked with extracting named entities from a given text. Named entities can be organizations, locations, dates, etc.\n",
    "    The output should be in the following JSON format, between 3 backticks:\n",
    "    ```\n",
    "    {{\"entities\": [\"ENTITY_A\", \"ENTITY_B\"]}}\n",
    "    ```\n",
    "    Consider the following examples:\n",
    "    {examples_text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Call OpenAI API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract response text\n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "\n",
    "    # Extract JSON safely\n",
    "    match = re.search(r'```json\\n(.*?)\\n```', response_text, re.DOTALL)\n",
    "    if match:\n",
    "        entities_json = match.group(1)\n",
    "        entities = json.loads(entities_json).get(\"entities\", [])\n",
    "    else:\n",
    "        entities = []\n",
    "\n",
    "    return list(set(entities))  # Return unique entities\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function to process the test data, find similar examples, and extract entities. \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    test_data = json.load(open(\"test_data.json\", \"r\"))\n",
    "    train_data = json.load(open(\"train_data.json\", \"r\"))\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(\"training_data_vectors.faiss\")\n",
    "\n",
    "    # Store predicted entities\n",
    "    predicted_entities_with_similarity = []\n",
    "\n",
    "    for text in test_data['text']:\n",
    "        # Generate embedding\n",
    "        embedding = helper.get_embeddings_oai([text])\n",
    "\n",
    "        # Ensure 2D shape for FAISS search\n",
    "        D, I = index.search(embedding.reshape(1, -1), 10)\n",
    "\n",
    "        # Get similar examples\n",
    "        similar_examples = [(train_data['text'][i], train_data['entities'][i]) for i in I[0]]\n",
    "\n",
    "        # Extract entities\n",
    "        entities = extract_entities(text, similar_examples)\n",
    "\n",
    "        # Store results\n",
    "        predicted_entities_with_similarity.append(entities)\n",
    "\n",
    "    # Save extracted entities\n",
    "    with open(\"predicted_entities_with_similarity.pkl\", \"wb\") as f:\n",
    "        pickle.dump(predicted_entities_with_similarity, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"def semantic_match(name1, name2, threshold=0.85):\n",
    "    \"\" Use Sentence Transformers for semantic similarity matching \"\n",
    "    similarity = util.pytorch_cos_sim(semantic_model.encode(name1, convert_to_tensor=True),\n",
    "                                      semantic_model.encode(name2, convert_to_tensor=True))\n",
    "    return similarity.item() >= threshold\"\"\"\n",
    "\n",
    "import openai\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def get_openai_embedding(text):\n",
    "    \"\"\"Get embeddings from OpenAI's API.\"\"\"\n",
    "    response = openai.Embedding.create(\n",
    "        model=\"text-embedding-ada-002\",  # Specify the OpenAI model\n",
    "        input=text\n",
    "    )\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "def semantic_match(name1, name2, threshold=0.85):\n",
    "    \"\"\"Use Sentence Transformers and OpenAI for semantic similarity matching.\"\"\"\n",
    "    try:\n",
    "        # Encode the names to get their embeddings\n",
    "        embedding1 = semantic_model.encode(name1, convert_to_tensor=True)\n",
    "        embedding2 = semantic_model.encode(name2, convert_to_tensor=True)\n",
    "\n",
    "        # Compute cosine similarity using Sentence Transformers\n",
    "        similarity_st = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "\n",
    "        # Get embeddings from OpenAI\n",
    "        openai_embedding1 = get_openai_embedding(name1)\n",
    "        openai_embedding2 = get_openai_embedding(name2)\n",
    "\n",
    "        # Compute cosine similarity using OpenAI embeddings\n",
    "        similarity_openai = util.pytorch_cos_sim(\n",
    "            torch.tensor(openai_embedding1),\n",
    "            torch.tensor(openai_embedding2)\n",
    "        ).item()\n",
    "\n",
    "        # Average the similarities from both models\n",
    "        average_similarity = (similarity_st + similarity_openai) / 2\n",
    "\n",
    "        # Return whether the average similarity meets the threshold\n",
    "        return average_similarity >= threshold\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in semantic_match: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "\"\"\"if __name__ == \"__main__\":\n",
    "    name1 = \"John Doe\"\n",
    "    name2 = \"Jonathan Doe\"\n",
    "    \n",
    "    if semantic_match(name1, name2):\n",
    "        print(f\"{name1} and {name2} are semantically similar.\")\n",
    "    else:\n",
    "        print(f\"{name1} and {name2} are not semantically similar.\")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"def geocode_location(location):\n",
    "    \" Get geographical coordinates using Google Maps API with caching. \"\n",
    "    if location in geo_cache:\n",
    "        return geo_cache[location]\n",
    "\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={location}&key={GOOGLE_API_KEY}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if data[\"status\"] == \"OK\":\n",
    "            lat = data[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "            lon = data[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "            geo_cache[location] = (lat, lon)\n",
    "            return lat, lon\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching geolocation for {location}: {e}\")\n",
    "    return None\"\"\"\n",
    "\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "import openai\n",
    "\n",
    "# Initialize the cache\n",
    "#geo_cache = {}\n",
    "\n",
    "# Replace with your actual Google Maps API key\n",
    "GOOGLE_API_KEY = OPENAI_API_KEY\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def geocode_location(location):\n",
    "    \"\"\" Get geographical coordinates using Google Maps API with caching and OpenAI integration. \"\"\"\n",
    "    if location in geo_cache:\n",
    "        logging.info(f\"Cache hit for location: {location}\")\n",
    "        return geo_cache[location]\n",
    "\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={location}&key={GOOGLE_API_KEY}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        data = response.json()\n",
    "        \n",
    "        if data[\"status\"] == \"OK\":\n",
    "            lat = data[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
    "            lon = data[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
    "            geo_cache[location] = (lat, lon)\n",
    "            logging.info(f\"Coordinates for {location}: ({lat}, {lon})\")\n",
    "            return lat, lon\n",
    "        else:\n",
    "            logging.error(f\"Error in response: {data['status']}\")\n",
    "            handle_openai_error(data['status'])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Request error for {location}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error fetching geolocation for {location}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def handle_openai_error(status):\n",
    "    \"\"\" Handle errors by sending a request to OpenAI for further analysis. \"\"\"\n",
    "    try:\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the geocoding error: {status}\"}\n",
    "            ]\n",
    "        )\n",
    "        logging.info(f\"OpenAI response: {response['choices'][0]['message']['content']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"OpenAI error: {e}\")\n",
    "\n",
    "# Example usage\n",
    "\"\"\"if __name__ == \"__main__\":\n",
    "    location = \"1600 Amphitheatre Parkway, Mountain View, CA\"\n",
    "    coordinates = geocode_location(location)\n",
    "    if coordinates:\n",
    "        print(f\"Coordinates for {location}: {coordinates}\")\n",
    "    else:\n",
    "        print(f\"Could not retrieve coordinates for {location}.\")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_context(entity):\n",
    "    \"\"\" Generate additional context about the entity using OpenAI API. \"\"\"\n",
    "    prompt = f\"Provide a brief description of {entity}.\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or any other model you prefer\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating context for {entity}: {e}\")\n",
    "        return None\n",
    "\n",
    "\"\"\"def process_text(text):\n",
    "    \" Process input text to extract, normalize, and geocode geospatial entities. \"\n",
    "    extracted_entities = extract_entities(text)\n",
    "    geo_entities = {}\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(geocode_location, entity): entity for entity in extracted_entities}\n",
    "        for future in futures:\n",
    "            entity = futures[future]\n",
    "            coordinates = future.result()\n",
    "            if coordinates:\n",
    "                # Check for duplicate variations using semantic similarity\n",
    "                matched = None\n",
    "                for existing in geo_entities:\n",
    "                    if semantic_match(existing, entity):\n",
    "                        matched = existing\n",
    "                        break\n",
    "                \n",
    "                if matched:\n",
    "                    continue  # Skip redundant entity\n",
    "                geo_entities[entity] = {\n",
    "                    \"coordinates\": coordinates,\n",
    "                    \"context\": generate_context(entity)  # Generate context for the entity\n",
    "                }\n",
    "    \n",
    "    return geo_entities\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Assuming these functions are defined elsewhere in your code\n",
    "# from your_module import extract_entities, geocode_location, semantic_match, generate_context\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\" Process input text to extract, normalize, and geocode geospatial entities. \"\"\"\n",
    "    extracted_entities = extract_entities(text)\n",
    "    geo_entities = {}\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(geocode_location, entity): entity for entity in extracted_entities}\n",
    "        for future in futures:\n",
    "            entity = futures[future]\n",
    "            try:\n",
    "                coordinates = future.result()\n",
    "                if coordinates:\n",
    "                    # Check for duplicate variations using semantic similarity\n",
    "                    matched = None\n",
    "                    for existing in geo_entities:\n",
    "                        if semantic_match(existing, entity):\n",
    "                            matched = existing\n",
    "                            break\n",
    "                    \n",
    "                    if matched:\n",
    "                        logging.info(f\"Duplicate entity found: {entity} is similar to {matched}. Skipping.\")\n",
    "                        continue  # Skip redundant entity\n",
    "                    \n",
    "                    # Generate context for the entity\n",
    "                    context = generate_context(entity)\n",
    "                    geo_entities[entity] = {\n",
    "                        \"coordinates\": coordinates,\n",
    "                        \"context\": context\n",
    "                    }\n",
    "                    logging.info(f\"Processed entity: {entity} with coordinates: {coordinates}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing entity {entity}: {e}\")\n",
    "    \n",
    "    return geo_entities\n",
    "\n",
    "# Example usage\n",
    "\"\"\"if __name__ == \"__main__\":\n",
    "    text = \"Visit the Eiffel Tower in Paris and the Statue of Liberty in New York.\"\n",
    "    result = process_text(text)\n",
    "    print(result)\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    texts = [\n",
    "        \"The wildfire in California has spread rapidly. Evacuations are in place for Los Angeles County.\",\n",
    "        \"New York and Washington DC are experiencing heavy snowfall.\",\n",
    "        \"A major earthquake struck Tokyo, Japan early this morning.\"\n",
    "    ]\n",
    "    ground_truths = [\n",
    "        [\"California\", \"Los Angeles County\"],\n",
    "        [\"New York\", \"Washington DC\"],\n",
    "        [\"Tokyo\", \"Japan\"]\n",
    "    ]\n",
    "\n",
    "    main()\n",
    "\n",
    "\n",
    "    coordinates = geocode_location(location)\n",
    "    if coordinates:\n",
    "        print(f\"Coordinates for {location}: {coordinates}\")\n",
    "    else:\n",
    "        print(f\"Could not retrieve coordinates for {location}.\")\n",
    "\n",
    "    if semantic_match(name1, name2):\n",
    "        print(f\"{name1} and {name2} are semantically similar.\")\n",
    "    else:\n",
    "        print(f\"{name1} and {name2} are not semantically similar.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    precision, recall, f1, accuracy = evaluate_model(test_texts, ground_truths)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    sample_text = \"The wildfire in Chennai has spread rapidly. Evacuations are in place for Karnataka state.\"\n",
    "    results = process_text(text)\n",
    "    print(\"Geospatial Entities and Coordinates:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
