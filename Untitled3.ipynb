{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57ccd74-6eb0-4cc2-94c0-365a20aa04a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folium in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.19.5)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from folium) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from folium) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from folium) (2.32.2)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\admin\\anaconda3\\lib\\site-packages (from folium) (2022.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->folium) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->folium) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->folium) (2024.8.30)\n",
      "Collecting geopy\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/125.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/125.4 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 30.7/125.4 kB 435.7 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 41.0/125.4 kB 281.8 kB/s eta 0:00:01\n",
      "   ------------------------- ------------- 81.9/125.4 kB 512.0 kB/s eta 0:00:01\n",
      "   ------------------------- ------------- 81.9/125.4 kB 512.0 kB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 92.2/125.4 kB 350.1 kB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 92.2/125.4 kB 350.1 kB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 92.2/125.4 kB 350.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 125.4/125.4 kB 307.2 kB/s eta 0:00:00\n",
      "Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.3 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 10.2/40.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.3/40.3 kB 654.0 kB/s eta 0:00:00\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.1\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.46.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install folium\n",
    "!pip install geopy\n",
    "!pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68b9d462-b663-454c-83f4-77549922b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: NLP for Entity Extraction\n",
    "def extract_location_and_disaster(text):\n",
    "    # Use a pre-trained NLP model for Named Entity Recognition (NER)\n",
    "    nlp = pipeline(\"ner\", grouped_entities=True)\n",
    "    entities = nlp(text)\n",
    "    \n",
    "    # Extract location and disaster type\n",
    "    locations = [entity['word'] for entity in entities if entity['entity_group'] == 'LOC']\n",
    "    disaster_types = [\"flood\", \"earthquake\", \"fire\", \"storm\"]  # Example disaster types\n",
    "    disaster = next((word for word in disaster_types if word in text.lower()), None)\n",
    "    \n",
    "    return locations, disaster\n",
    "\n",
    "# Step 2: Geospatial AI for Location Mapping\n",
    "def geocode_location(location_name):\n",
    "    geolocator = Nominatim(user_agent=\"geo_ai_system\")\n",
    "    location = geolocator.geocode(location_name)\n",
    "    if location:\n",
    "        return (location.latitude, location.longitude)\n",
    "    return None\n",
    "\n",
    "def create_map(locations, disaster_type):\n",
    "    # Create a map centered on the first location\n",
    "    map_center = locations[0]\n",
    "    disaster_map = folium.Map(location=map_center, zoom_start=12)\n",
    "    \n",
    "    # Add markers for each location\n",
    "    for loc in locations:\n",
    "        folium.Marker(loc, popup=f\"{disaster_type.capitalize()} here\").add_to(disaster_map)\n",
    "    \n",
    "    # Highlight affected area\n",
    "    folium.Circle(locations[0], radius=1000, color='red', fill=True, fill_color='red').add_to(disaster_map)\n",
    "    \n",
    "    return disaster_map\n",
    "\n",
    "# Step 3: Real-Time Vehicle Tracking (Simulated)\n",
    "def track_vehicles(vehicle_locations):\n",
    "    vehicle_map = folium.Map(location=vehicle_locations[0], zoom_start=12)\n",
    "    for idx, loc in enumerate(vehicle_locations):\n",
    "        folium.Marker(loc, popup=f\"Vehicle {idx + 1}\").add_to(vehicle_map)\n",
    "    return vehicle_map\n",
    "\n",
    "# Step 4: Damage Prediction (Simulated)\n",
    "def predict_damage(affected_area):\n",
    "    # Simulate damage prediction based on distance from the epicenter\n",
    "    damage_zones = {\n",
    "        \"High Risk\": 500,  # Within 500 meters\n",
    "        \"Medium Risk\": 1000,  # Within 1 km\n",
    "        \"Low Risk\": 2000  # Within 2 km\n",
    "    }\n",
    "    return damage_zones\n",
    "\n",
    "# Step 5: Recommendation Engine (Simulated)\n",
    "def generate_recommendations(disaster_type, damage_zones):\n",
    "    recommendations = {\n",
    "        \"High Risk\": f\"Evacuate immediately. Send emergency vehicles to {disaster_type} area.\",\n",
    "        \"Medium Risk\": \"Prepare for evacuation. Monitor the situation closely.\",\n",
    "        \"Low Risk\": \"Stay alert. No immediate action required.\"\n",
    "    }\n",
    "    return recommendations\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Example input text\n",
    "    text = \"A major flood has been reported in Chennai. Main Street is completely submerged.\"\n",
    "    \n",
    "    # Step 1: Extract location and disaster type\n",
    "    locations, disaster_type = extract_location_and_disaster(text)\n",
    "    print(f\"Extracted Locations: {locations}\")\n",
    "    print(f\"Disaster Type: {disaster_type}\")\n",
    "    \n",
    "    # Step 2: Geocode locations and create a map\n",
    "    geocoded_locations = [geocode_location(loc) for loc in locations if geocode_location(loc)]\n",
    "    if geocoded_locations:\n",
    "        disaster_map = create_map(geocoded_locations, disaster_type)\n",
    "        disaster_map.save(\"disaster_map.html\")\n",
    "        print(\"Disaster map saved as 'disaster_map.html'.\")\n",
    "    \n",
    "    # Step 3: Simulate vehicle tracking\n",
    "    vehicle_locations = [(40.7128, -74.0060), (40.7306, -73.9352)]  # Example vehicle GPS coordinates\n",
    "    vehicle_map = track_vehicles(vehicle_locations)\n",
    "    vehicle_map.save(\"vehicle_map.html\")\n",
    "    print(\"Vehicle map saved as 'vehicle_map.html'.\")\n",
    "    \n",
    "    # Step 4: Predict damage\n",
    "    damage_zones = predict_damage(geocoded_locations[0])\n",
    "    print(\"Damage Zones:\", damage_zones)\n",
    "    \n",
    "    # Step 5: Generate recommendations\n",
    "    recommendations = generate_recommendations(disaster_type, damage_zones)\n",
    "    print(\"Recommendations:\", recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a58a13cb-1f91-4e2c-a1b7-be6f20761526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Locations: ['Chennai', 'Main Street']\n",
      "Disaster Type: flood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 10:57:30,509 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\")': /search?q=Main+Street&format=json&limit=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disaster map saved as 'disaster_map.html'.\n",
      "Vehicle map saved as 'vehicle_map.html'.\n",
      "Damage Zones: {'High Risk': 500, 'Medium Risk': 1000, 'Low Risk': 2000}\n",
      "Recommendations: {'High Risk': 'Evacuate immediately. Send emergency vehicles to flood area.', 'Medium Risk': 'Prepare for evacuation. Monitor the situation closely.', 'Low Risk': 'Stay alert. No immediate action required.'}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34109339-9a16-4b6a-9016-a76a439fd298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka\n",
      "  Downloading kafka-1.3.5-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Downloading kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "   ---------------------------------------- 0.0/207.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/207.2 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 20.5/207.2 kB 330.3 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 30.7/207.2 kB 330.3 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 61.4/207.2 kB 409.6 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 92.2/207.2 kB 476.3 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 163.8/207.2 kB 701.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 207.2/207.2 kB 787.8 kB/s eta 0:00:00\n",
      "Installing collected packages: kafka\n",
      "Successfully installed kafka-1.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800e0a6a-7bea-4052-a88b-8a9080529e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\thinc\\shims\\pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 21:00:49,937 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8000\n",
      " * Running on http://192.168.31.219:8000\n",
      "2025-05-07 21:00:49,938 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2025-05-07 21:00:49,959 - INFO -  * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import openai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "#from kafka import KafkaProducer, KafkaConsumer\n",
    "from threading import Thread\n",
    "import os\n",
    "\n",
    "# Load NLP Models\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp_ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# API Keys\n",
    "GOOGLE_API_KEY = \"YOUR_GOOGLE_API_KEY\"\n",
    "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Kafka Setup\n",
    "#producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "#consumer = KafkaConsumer('geo-disaster-stream', bootstrap_servers='localhost:9092', auto_offset_reset='earliest', group_id='geo-nlp-group', value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "\n",
    "# Cache for geocoding results\n",
    "geo_cache = {}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Flask App\n",
    "app = Flask(__name__, template_folder=\"templates\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/analyze\", methods=[\"POST\"])\n",
    "def analyze_disaster():\n",
    "    data = request.get_json()\n",
    "    result = process_text(data['text'])\n",
    "    producer.send(\"geo-disaster-stream\", result)\n",
    "    return jsonify(result)\n",
    "\n",
    "def extract_entities(text: str, examples: List[Tuple[str, List[str]]]) -> List[str]:\n",
    "    examples_text = \"\\n\".join([f\"Text: {ex[0]}\\nExtracted Entities: {ex[1]}\" for ex in examples])\n",
    "    system_prompt = f\"\"\"\n",
    "    Extract named entities from the text. Return JSON format between 3 backticks:\n",
    "    ```\n",
    "    {{\"entities\": [\"ENTITY_A\", \"ENTITY_B\"]}}\n",
    "    ```\n",
    "    Examples:\n",
    "    {examples_text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ]\n",
    "        )\n",
    "        response_text = response['choices'][0]['message']['content']\n",
    "        match = re.search(r'```json\\n(.*?)\\n```', response_text, re.DOTALL)\n",
    "        if match:\n",
    "            return json.loads(match.group(1)).get(\"entities\", [])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting entities: {e}\")\n",
    "    return []\n",
    "\n",
    "def geocode_location(location: str) -> Optional[Tuple[float, float]]:\n",
    "    if location in geo_cache:\n",
    "        return geo_cache[location]\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={location}&key={GOOGLE_API_KEY}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data[\"status\"] == \"OK\":\n",
    "            lat, lon = data[\"results\"][0][\"geometry\"][\"location\"].values()\n",
    "            geo_cache[location] = (lat, lon)\n",
    "            return lat, lon\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error geocoding {location}: {e}\")\n",
    "    return None\n",
    "\n",
    "def semantic_match(name1: str, name2: str, threshold: float = 0.85) -> bool:\n",
    "    similarity = util.pytorch_cos_sim(\n",
    "        semantic_model.encode(name1, convert_to_tensor=True),\n",
    "        semantic_model.encode(name2, convert_to_tensor=True)\n",
    "    ).item()\n",
    "    return similarity >= threshold\n",
    "\n",
    "def process_text(text: str) -> Dict[str, Dict[str, Tuple[float, float]]]:\n",
    "    extracted_entities = extract_entities(text, [])\n",
    "    geo_entities = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(geocode_location, entity): entity for entity in extracted_entities}\n",
    "        for future in as_completed(futures):\n",
    "            entity = futures[future]\n",
    "            try:\n",
    "                coordinates = future.result()\n",
    "                if coordinates and not any(semantic_match(existing, entity) for existing in geo_entities):\n",
    "                    geo_entities[entity] = {\"coordinates\": coordinates}\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing entity {entity}: {e}\")\n",
    "    return geo_entities\n",
    "\n",
    "'''def kafka_consumer():\n",
    "    for message in consumer:\n",
    "        data = message.value\n",
    "        logging.info(f\"Consumed from Kafka: {data}\")\n",
    "\n",
    "Thread(target=kafka_consumer, daemon=True).start()'''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"templates\", exist_ok=True)\n",
    "    with open(\"templates/index.html\", \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\\\"en\\\">\n",
    "        <head>\n",
    "            <meta charset=\\\"UTF-8\\\">\n",
    "            <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\n",
    "            <title>Geo NLP Analyzer</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Geo-NLP Disaster Analyzer</h1>\n",
    "            <form id=\\\"analyzeForm\\\">\n",
    "                <textarea id=\\\"text\\\" rows=\\\"10\\\" cols=\\\"50\\\" placeholder=\\\"Enter disaster report...\\\"></textarea><br>\n",
    "                <button type=\\\"submit\\\">Analyze</button>\n",
    "            </form>\n",
    "            <pre id=\\\"output\\\"></pre>\n",
    "            <script>\n",
    "                document.getElementById('analyzeForm').addEventListener('submit', async (e) => {\n",
    "                    e.preventDefault();\n",
    "                    const text = document.getElementById('text').value;\n",
    "                    const res = await fetch('/analyze', {\n",
    "                        method: 'POST',\n",
    "                        headers: { 'Content-Type': 'application/json' },\n",
    "                        body: JSON.stringify({ text })\n",
    "                    });\n",
    "                    const data = await res.json();\n",
    "                    document.getElementById('output').textContent = JSON.stringify(data, null, 2);\n",
    "                });\n",
    "            </script>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\")\n",
    "    app.run(host=\"0.0.0.0\", port=8000, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc36a1-c026-464e-bbba-5031cd323a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
